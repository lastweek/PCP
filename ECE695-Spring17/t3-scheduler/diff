diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/.gitignore b/ECE695-Spring17/t3-scheduler/linux-4.10.6/.gitignore
index c2ed4ec..0f76647 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/.gitignore
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/.gitignore
@@ -68,6 +68,7 @@ Module.symvers
 !.gitignore
 !.mailmap
 !.cocciconfig
+!.config
 
 #
 # Generated include files
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/arch/x86/entry/syscalls/syscall_64.tbl b/ECE695-Spring17/t3-scheduler/linux-4.10.6/arch/x86/entry/syscalls/syscall_64.tbl
index e93ef0b..0fea2d2 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/arch/x86/entry/syscalls/syscall_64.tbl
@@ -338,6 +338,7 @@
 329	common	pkey_mprotect		sys_pkey_mprotect
 330	common	pkey_alloc		sys_pkey_alloc
 331	common	pkey_free		sys_pkey_free
+332	common	sched_setlimit		sys_sched_setlimit
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/linux/sched.h b/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/linux/sched.h
index ad3ec9e..6cfe3ab 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/linux/sched.h
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/linux/sched.h
@@ -1377,6 +1377,7 @@ struct sched_entity {
 	struct list_head	group_node;
 	unsigned int		on_rq;
 
+	u64			mycfs_limit;
 	u64			exec_start;
 	u64			sum_exec_runtime;
 	u64			vruntime;
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/linux/syscalls.h b/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/linux/syscalls.h
index 91a740f..69fe081 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/linux/syscalls.h
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/linux/syscalls.h
@@ -903,4 +903,7 @@ asmlinkage long sys_pkey_mprotect(unsigned long start, size_t len,
 asmlinkage long sys_pkey_alloc(unsigned long flags, unsigned long init_val);
 asmlinkage long sys_pkey_free(int pkey);
 
+/* For MyCFS */
+asmlinkage long sys_sched_setlimit(pid_t pid, int limit);
+
 #endif
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/uapi/linux/sched.h b/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/uapi/linux/sched.h
index 5f0fe01..3561b7c 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/uapi/linux/sched.h
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/include/uapi/linux/sched.h
@@ -39,6 +39,7 @@
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
 #define SCHED_DEADLINE		6
+#define SCHED_MYCFS		7
 
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/Makefile b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/Makefile
index 5e59b83..39a6529 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/Makefile
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/Makefile
@@ -17,6 +17,7 @@ endif
 
 obj-y += core.o loadavg.o clock.o cputime.o
 obj-y += idle_task.o fair.o rt.o deadline.o stop_task.o
+obj-y += mycfs.o
 obj-y += wait.o swait.o completion.o idle.o
 obj-$(CONFIG_SMP) += cpupri.o cpudeadline.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/core.c b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/core.c
index c56fb57..3214eba 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/core.c
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/core.c
@@ -2412,6 +2412,10 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		return -EAGAIN;
 	} else if (rt_prio(p->prio)) {
 		p->sched_class = &rt_sched_class;
+	} else if (mycfs_policy(p->policy)) {
+		pr_info("%s(CPU%d): pid:%d is going to use mycfs, cur:%d",
+			__func__, smp_processor_id(), p->pid, current->pid);
+		p->sched_class = &mycfs_sched_class;
 	} else {
 		p->sched_class = &fair_sched_class;
 	}
@@ -3954,6 +3958,8 @@ static void __setscheduler_params(struct task_struct *p,
 		__setparam_dl(p, attr);
 	else if (fair_policy(policy))
 		p->static_prio = NICE_TO_PRIO(attr->sched_nice);
+	else if (mycfs_policy(policy))
+		p->static_prio = NICE_TO_PRIO(attr->sched_nice);
 
 	/*
 	 * __sched_setscheduler() ensures attr->sched_priority == 0 when
@@ -3984,7 +3990,11 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
 		p->sched_class = &dl_sched_class;
 	else if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
-	else
+	else if (mycfs_policy(attr->sched_policy)) {
+		p->sched_class = &mycfs_sched_class;
+		pr_info("%s(CPU%d): %d/%s change to MyCFS",
+			__func__, smp_processor_id(), p->pid, p->comm);
+	} else
 		p->sched_class = &fair_sched_class;
 }
 
@@ -4474,11 +4484,13 @@ err_size:
 SYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy,
 		struct sched_param __user *, param)
 {
+	int ret;
 	/* negative values for policy are not valid */
 	if (policy < 0)
 		return -EINVAL;
 
-	return do_sched_setscheduler(pid, policy, param);
+	ret = do_sched_setscheduler(pid, policy, param);
+	return ret;
 }
 
 /**
@@ -7614,6 +7626,7 @@ void __init sched_init(void)
 		rq->nr_running = 0;
 		rq->calc_load_active = 0;
 		rq->calc_load_update = jiffies + LOAD_FREQ;
+		init_mycfs_rq(&rq->mycfs);
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt);
 		init_dl_rq(&rq->dl);
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/debug.c b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/debug.c
index fa178b6..af88771 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/debug.c
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/debug.c
@@ -470,6 +470,42 @@ static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
 	rcu_read_unlock();
 }
 
+void print_mycfs_stats(struct seq_file *m, int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	struct mycfs_rq *mycfs_rq = &rq->mycfs;
+	s64 MIN_vruntime = -1, min_vruntime, max_vruntime = -1, spread;
+	struct sched_entity *last;
+	unsigned long flags;
+
+	SEQ_printf(m, "\nmycfs_rq[%d]:\n", cpu);
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "exec_clock",
+			SPLIT_NS(mycfs_rq->exec_clock));
+
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	if (mycfs_rq->rb_leftmost)
+		MIN_vruntime = (__pick_first_entity_mycfs(mycfs_rq))->vruntime;
+	last = __pick_last_entity_mycfs(mycfs_rq);
+	if (last)
+		max_vruntime = last->vruntime;
+	min_vruntime = mycfs_rq->min_vruntime;
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "MIN_vruntime",
+			SPLIT_NS(MIN_vruntime));
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "min_vruntime",
+			SPLIT_NS(min_vruntime));
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "max_vruntime",
+			SPLIT_NS(max_vruntime));
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "period",
+			SPLIT_NS(mycfs_rq->period));
+	spread = max_vruntime - MIN_vruntime;
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "spread",
+			SPLIT_NS(spread));
+	SEQ_printf(m, "  .%-30s: %d\n", "nr_running", mycfs_rq->nr_running);
+	SEQ_printf(m, "  .%-30s: %ld\n", "load", mycfs_rq->load.weight);
+}
+
 void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 {
 	s64 MIN_vruntime = -1, min_vruntime, max_vruntime = -1,
@@ -643,6 +679,7 @@ do {									\
 #undef P
 
 	spin_lock_irqsave(&sched_debug_lock, flags);
+	print_mycfs_stats(m, cpu);
 	print_cfs_stats(m, cpu);
 	print_rt_stats(m, cpu);
 	print_dl_stats(m, cpu);
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/fair.c b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/fair.c
index 6559d19..c01887b 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/fair.c
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/fair.c
@@ -75,7 +75,7 @@ unsigned int normalized_sysctl_sched_min_granularity	= 750000ULL;
 /*
  * This value is kept at sysctl_sched_latency/sysctl_sched_min_granularity
  */
-static unsigned int sched_nr_latency = 8;
+unsigned int sched_nr_latency = 8;
 
 /*
  * After fork, child runs first. If set to 0 (default) then
@@ -6220,6 +6220,9 @@ pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct pin_cookie c
 	struct task_struct *p;
 	int new_tasks;
 
+	if (unlikely(task_has_mycfs_policy(prev)))
+		return NULL;
+
 again:
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	if (!cfs_rq->nr_running)
@@ -9400,7 +9403,7 @@ static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task
  * All the scheduling class methods:
  */
 const struct sched_class fair_sched_class = {
-	.next			= &idle_sched_class,
+	.next			= &mycfs_sched_class,
 	.enqueue_task		= enqueue_task_fair,
 	.dequeue_task		= dequeue_task_fair,
 	.yield_task		= yield_task_fair,
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/mycfs.c b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/mycfs.c
new file mode 100644
index 0000000..165ed28
--- /dev/null
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/mycfs.c
@@ -0,0 +1,1165 @@
+/*
+ * SCHED_MYCFS class, modified after CFS
+ * For Spring 2017 ECE695 task 3
+ *
+ * Yizhou Shan, March 2017
+ */
+
+/*
+ * Note:
+ *
+ * 0) mycfs_rq->curr is not kept in the rb-tree
+ *
+ * 1) When a task call sched_setscheduler() to switch to MyCFS,
+ *    the task will be dequeued and put from previous class,
+ *    after that it will call enqueue_task() and set_curr_task()
+ *    to put the task into MyCFS class. After all this, the
+ *    switched_to_mycfs() callback will be invoked.
+ *
+ */
+
+#define pr_fmt(fmt) "MyCFS: " fmt
+
+#include <linux/sched.h>
+#include <linux/latencytop.h>
+#include <linux/cpumask.h>
+#include <linux/cpuidle.h>
+#include <linux/slab.h>
+#include <linux/profile.h>
+#include <linux/interrupt.h>
+#include <linux/mempolicy.h>
+#include <linux/migrate.h>
+#include <linux/task_work.h>
+#include <linux/syscalls.h>
+
+#include <trace/events/sched.h>
+
+#include "sched.h"
+
+static int mycfs_debug = 0;
+
+static int __init mycfs_set_debug(char *arg)
+{
+	mycfs_debug = 1;
+	return 0;
+}
+__setup("mycfs_debug", mycfs_set_debug);
+
+#define mycfs_printk(s, a...)					\
+do {								\
+	if (mycfs_debug) {					\
+		pr_info("%s(CPU%d): ",				\
+			__func__, smp_processor_id());		\
+		pr_cont(s, ##a);				\
+	}							\
+} while (0)
+
+/* By default, period is 100ms */
+#define MYCFS_PREIOD	100000ULL
+
+static inline bool has_limit(struct mycfs_rq *mycfs_rq,
+			     struct sched_entity *se)
+{
+	if (se->mycfs_limit != 0 && se->mycfs_limit < mycfs_rq->period)
+		return true;
+	return false;
+}
+
+static inline struct task_struct *task_of(struct sched_entity *se)
+{
+	return container_of(se, struct task_struct, se);
+}
+
+static inline struct rq *rq_of(struct mycfs_rq *mycfs_rq)
+{
+	return container_of(mycfs_rq, struct rq, mycfs);
+}
+
+static inline struct mycfs_rq *task_mycfs_rq(struct task_struct *p)
+{
+	return &task_rq(p)->mycfs;
+}
+
+static inline struct mycfs_rq *mycfs_rq_of(struct sched_entity *se)
+{
+	struct task_struct *p = task_of(se);
+	struct rq *rq = task_rq(p);
+
+	return &rq->mycfs;
+}
+
+static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)
+{
+	s64 delta = (s64)(vruntime - max_vruntime);
+	if (delta > 0)
+		max_vruntime = vruntime;
+
+	return max_vruntime;
+}
+
+static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)
+{
+	s64 delta = (s64)(vruntime - min_vruntime);
+	if (delta < 0)
+		min_vruntime = vruntime;
+
+	return min_vruntime;
+}
+
+static inline int entity_before(struct sched_entity *a,
+				struct sched_entity *b)
+{
+	return (s64)(a->vruntime - b->vruntime) < 0;
+}
+
+static void account_entity_enqueue(struct mycfs_rq *mycfs_rq,
+				   struct sched_entity *se)
+{
+	mycfs_rq->nr_running++;
+}
+
+static void account_entity_dequeue(struct mycfs_rq *mycfs_rq,
+				   struct sched_entity *se)
+{
+	mycfs_rq->nr_running--;
+}
+
+/*
+ * Virtual runtime, wall-time, and weight manipulations
+ */
+
+#define WMULT_CONST	(~0U)
+#define WMULT_SHIFT	32
+
+static void __update_inv_weight(struct load_weight *lw)
+{
+	unsigned long w;
+
+	if (likely(lw->inv_weight))
+		return;
+
+	w = scale_load_down(lw->weight);
+
+	if (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))
+		lw->inv_weight = 1;
+	else if (unlikely(!w))
+		lw->inv_weight = WMULT_CONST;
+	else
+		lw->inv_weight = WMULT_CONST / w;
+}
+
+/*
+ * delta_exec * weight / lw.weight
+ *   OR
+ * (delta_exec * (weight * lw->inv_weight)) >> WMULT_SHIFT
+ *
+ * Either weight := NICE_0_LOAD and lw \e sched_prio_to_wmult[], in which case
+ * we're guaranteed shift stays positive because inv_weight is guaranteed to
+ * fit 32 bits, and NICE_0_LOAD gives another 10 bits; therefore shift >= 22.
+ *
+ * Or, weight =< lw.weight (because lw.weight is the runqueue weight), thus
+ * weight/lw.weight <= 1, and therefore our shift will also be positive.
+ */
+static u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)
+{
+	u64 fact = scale_load_down(weight);
+	int shift = WMULT_SHIFT;
+
+	__update_inv_weight(lw);
+
+	if (unlikely(fact >> 32)) {
+		while (fact >> 32) {
+			fact >>= 1;
+			shift--;
+		}
+	}
+
+	/* hint to use a 32x32->64 mul */
+	fact = (u64)(u32)fact * lw->inv_weight;
+
+	while (fact >> 32) {
+		fact >>= 1;
+		shift--;
+	}
+
+	return mul_u64_u32_shr(delta_exec, fact, shift);
+}
+
+/*
+ * The idea is to set a period in which each task runs once.
+ *
+ * When there are too many tasks (sched_nr_latency) we have to stretch
+ * this period because otherwise the slices get too small.
+ *
+ * p = (nr <= nl) ? l : l*nr/nl
+ */
+static u64 __sched_period(unsigned long nr_running)
+{
+	if (unlikely(nr_running > sched_nr_latency))
+		return nr_running * sysctl_sched_min_granularity;
+	else
+		return sysctl_sched_latency;
+}
+
+static inline void update_load_add(struct load_weight *lw, unsigned long inc)
+{
+	lw->weight += inc;
+	lw->inv_weight = 0;
+}
+
+/*
+ * Calculate the [wall-time slice] from the period by taking a part
+ * proportional to the weight:
+ *
+ * s = p*P[w/rw]
+ */
+static u64 sched_slice(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	u64 slice = __sched_period(mycfs_rq->nr_running + !se->on_rq);
+	struct load_weight *load = &mycfs_rq->load;
+	struct load_weight lw;
+
+	if (unlikely(!se->on_rq)) {
+		lw = mycfs_rq->load;
+
+		update_load_add(&lw, se->load.weight);
+		load = &lw;
+	}
+	slice = __calc_delta(slice, se->load.weight, load);
+
+	mycfs_printk("slice:%Lu,mycfs->min_vrt:%Lu,se->vrt:%Lu\n",
+		slice, mycfs_rq->min_vruntime, se->vruntime);
+
+	return slice;
+}
+
+/**
+ * calc_delta_mycfs	-	delta /= weight
+ * @delta: physical time in nanoseconads
+ * @se: the schedule entity
+ *
+ * Calculate vruntime base on physical time and weight
+ * Currently, we assume everything is equal weight
+ */
+static inline u64 calc_delta_mycfs(u64 delta, struct sched_entity *se)
+{
+#if 0
+	if (unlikely(se->load.weight != NICE_0_LOAD))
+		delta = __calc_delta(delta, NICE_0_LOAD, &se->load);
+#endif
+	return delta;
+}
+
+/*
+ * We calculate the vruntime slice of a to-be-inserted task.
+ *
+ * vs = s/w
+ */
+static u64 sched_vslice(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	return calc_delta_mycfs(sched_slice(mycfs_rq, se), se);
+}
+
+/*
+ * place_entity		-	Caculate the vruntime before placing into rbtree
+ *
+ * 1) A newly forked task will have some extra vruntime so it will not preempt
+ *    'current' very shortly. This keep the promises we made to 'current'.
+ * 
+ * 2) Minus the sched_latency from se->vruntime to give sleepers higher priority
+ */
+static void place_entity(struct mycfs_rq *mycfs_rq, struct sched_entity *se,
+			 int initial)
+{
+	u64 vruntime = mycfs_rq->min_vruntime;
+
+	/*
+	 * The 'current' period is already promised to the current tasks,
+	 * however the extra weight of the new task will slow them down a
+	 * little, place the new task so that it fits in the slot that
+	 * stays open at the end.
+	 */
+	if (initial && sched_feat(START_DEBIT))
+		vruntime += sched_vslice(mycfs_rq, se);
+
+	/* sleeps up to a single latency don't count. */
+	if (!initial) {
+		unsigned long thresh = sysctl_sched_latency;
+
+		/*
+		 * Halve their sleep time's effect, to allow
+		 * for a gentler effect of sleepers:
+		 */
+		if (sched_feat(GENTLE_FAIR_SLEEPERS))
+			thresh >>= 1;
+
+		vruntime -= thresh;
+	}
+
+	/* ensure we never gain time by being placed backwards. */
+	se->vruntime = max_vruntime(se->vruntime, vruntime);
+}
+
+static inline void set_last_buddy(struct sched_entity *se)
+{
+	mycfs_rq_of(se)->last = se;
+}
+
+static inline void set_next_buddy(struct sched_entity *se)
+{
+	mycfs_rq_of(se)->next = se;
+}
+
+static inline void set_skip_buddy(struct sched_entity *se)
+{
+	mycfs_rq_of(se)->skip = se;
+}
+
+static inline void __clear_buddies_last(struct sched_entity *se)
+{
+	mycfs_rq_of(se)->last = NULL;
+}
+
+static inline void __clear_buddies_next(struct sched_entity *se)
+{
+	mycfs_rq_of(se)->next = NULL;
+}
+
+static inline void __clear_buddies_skip(struct sched_entity *se)
+{
+	mycfs_rq_of(se)->skip = NULL;
+}
+
+static void clear_buddies(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	if (mycfs_rq->last == se)
+		__clear_buddies_last(se);
+
+	if (mycfs_rq->next == se)
+		__clear_buddies_next(se);
+
+	if (mycfs_rq->skip == se)
+		__clear_buddies_skip(se);
+}
+
+static void __enqueue_entity(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	struct rb_node **link = &mycfs_rq->tasks_timeline.rb_node;
+	struct rb_node *parent = NULL;
+	struct sched_entity *entry;
+	int leftmost = 1;
+
+	mycfs_printk("pid:%d", task_of(se)->pid);
+
+	/* Find the right place in the rbtree: */
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct sched_entity, run_node);
+		/*
+		 * We dont care about collisions. Nodes with
+		 * the same key stay together.
+		 */
+		if (entity_before(se, entry)) {
+			link = &parent->rb_left;
+		} else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	/*
+	 * Maintain a cache of leftmost tree entries
+	 * (it is frequently used):
+	 */
+	if (leftmost)
+		mycfs_rq->rb_leftmost = &se->run_node;
+
+	rb_link_node(&se->run_node, parent, link);
+	rb_insert_color(&se->run_node, &mycfs_rq->tasks_timeline);
+}
+
+static void __dequeue_entity(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	mycfs_printk("pid:%d", task_of(se)->pid);
+
+	if (mycfs_rq->rb_leftmost == &se->run_node) {
+		struct rb_node *next_node;
+
+		next_node = rb_next(&se->run_node);
+		mycfs_rq->rb_leftmost = next_node;
+	}
+
+	rb_erase(&se->run_node, &mycfs_rq->tasks_timeline);
+}
+
+struct sched_entity *__pick_first_entity_mycfs(struct mycfs_rq *mycfs_rq)
+{
+	struct rb_node *left = mycfs_rq->rb_leftmost;
+
+	if (!left)
+		return NULL;
+
+	return rb_entry(left, struct sched_entity, run_node);
+}
+
+struct sched_entity *__pick_last_entity_mycfs(struct mycfs_rq *mycfs_rq)
+{
+	struct rb_node *last = rb_last(&mycfs_rq->tasks_timeline);
+
+	if (!last)
+		return NULL;
+
+	return rb_entry(last, struct sched_entity, run_node);
+}
+
+static struct sched_entity *__pick_next_entity(struct sched_entity *se)
+{
+	struct rb_node *next = rb_next(&se->run_node);
+
+	if (!next)
+		return NULL;
+
+	return rb_entry(next, struct sched_entity, run_node);
+}
+
+/**
+ * update_min_vruntime		-	Update mycfs_rq->min_vruntime
+ *
+ * Find the minimum vruntime of current task and leftmost task in runqueue.
+ * Set this runtime as min_vruntime if it is greater than current value of
+ * min_vruntime:
+ */
+static void update_min_vruntime(struct mycfs_rq *mycfs_rq)
+{
+	struct sched_entity *curr = mycfs_rq->curr;
+	u64 vruntime = mycfs_rq->min_vruntime;
+
+	if (curr) {
+		if (curr->on_rq)
+			vruntime = curr->vruntime;
+		else
+			curr = NULL;
+	}
+
+	if (mycfs_rq->rb_leftmost) {
+		struct sched_entity *se = rb_entry(mycfs_rq->rb_leftmost,
+						   struct sched_entity,
+						   run_node);
+
+		if (!curr)
+			vruntime = se->vruntime;
+		else
+			vruntime = min_vruntime(vruntime, se->vruntime);
+	}
+
+	/* ensure we never gain time by being placed backwards. */
+	mycfs_rq->min_vruntime = max_vruntime(mycfs_rq->min_vruntime, vruntime);
+}
+
+/*
+ * Update the current task's runtime statistics, including
+ *	- se->exec_start
+ *	- se->sum_exec_runtime
+ *	- se->vruntime
+ *	- mycfs_rq->min_vruntime
+ */
+static void update_curr(struct mycfs_rq *mycfs_rq)
+{
+	struct sched_entity *curr = mycfs_rq->curr;
+	u64 now = rq_clock_task(rq_of(mycfs_rq));
+	u64 delta_exec;
+
+	if (unlikely(!curr))
+		return;
+
+	delta_exec = now - curr->exec_start;
+	if (unlikely((s64)delta_exec <= 0))
+		return;
+
+	curr->exec_start = now;
+
+	schedstat_set(curr->statistics.exec_max,
+		      max(delta_exec, curr->statistics.exec_max));
+
+	curr->sum_exec_runtime += delta_exec;
+
+	curr->vruntime += calc_delta_mycfs(delta_exec, curr);
+	update_min_vruntime(mycfs_rq);
+}
+
+static void update_curr_mycfs(struct rq *rq)
+{
+	update_curr(mycfs_rq_of(&rq->curr->se));
+}
+
+static void enqueue_entity(struct mycfs_rq *mycfs_rq, struct sched_entity *se, int flags)
+{
+	bool renorm = !(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_MIGRATED);
+	bool curr = mycfs_rq->curr == se;
+
+	/*
+	 * If we're the current task, we must renormalise before calling
+	 * update_curr().
+	 */
+	if (renorm && curr)
+		se->vruntime += mycfs_rq->min_vruntime;
+
+	update_curr(mycfs_rq);
+
+	/*
+	 * Otherwise, renormalise after, such that we're placed at the current
+	 * moment in time, instead of some random moment in the past. Being
+	 * placed in the past could significantly boost this task to the
+	 * fairness detriment of existing tasks.
+	 */
+	if (renorm && !curr)
+		se->vruntime += mycfs_rq->min_vruntime;
+
+	account_entity_enqueue(mycfs_rq, se);
+
+	if (flags & ENQUEUE_WAKEUP)
+		place_entity(mycfs_rq, se, 0);
+
+	/* curr is not kept in rbtree */
+	if (!curr)
+		__enqueue_entity(mycfs_rq, se);
+	se->on_rq = 1;
+
+	mycfs_printk("pid:%d,state:%ld,p->on_rq:%d,se->on_rq:%d,se->vrt:%Lu",
+		task_of(se)->pid, task_of(se)->state, task_of(se)->on_rq,
+		se->on_rq, se->vruntime);
+}
+
+/*
+ * The enqueue_task method is called before nr_running is increased.
+ * Here we update the mycfs scheduling stats and then put the task
+ * into the rbtree:
+ */
+static void enqueue_task_mycfs(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct sched_entity *se = &p->se;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+
+	enqueue_entity(mycfs_rq, se, flags);
+	mycfs_rq->h_nr_running++;
+	add_nr_running(rq, 1);
+}
+
+static void dequeue_entity(struct mycfs_rq *mycfs_rq, struct sched_entity *se,
+			   int flags)
+{
+	/*
+	 * Update run-time statistics of the 'current'.
+	 */
+	update_curr(mycfs_rq);
+
+	clear_buddies(mycfs_rq, se);
+
+	if (se != mycfs_rq->curr)
+		__dequeue_entity(mycfs_rq, se);
+	se->on_rq = 0;
+	account_entity_dequeue(mycfs_rq, se);
+
+	/*
+	 * Normalize after update_curr(); which will also have moved
+	 * min_vruntime if @se is the one holding it back. But before doing
+	 * update_min_vruntime() again, which will discount @se's position and
+	 * can move min_vruntime forward still more.
+	 */
+	if (!(flags & DEQUEUE_SLEEP))
+		se->vruntime -= mycfs_rq->min_vruntime;
+
+	/*
+	 * Now advance min_vruntime if @se was the entity holding it back,
+	 * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be
+	 * put back on, and if we advance min_vruntime, we'll be placed back
+	 * further than we started -- ie. we'll be penalized.
+	 */
+	if ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) == DEQUEUE_SAVE)
+		update_min_vruntime(mycfs_rq);
+
+	mycfs_printk("pid:%d,state:%ld,p->on_rq:%d,se->on_rq:%d,se->vrt:%Lu",
+		task_of(se)->pid, task_of(se)->state, task_of(se)->on_rq,
+		se->on_rq, se->vruntime);
+}
+
+/*
+ * The dequeue_task method is called before nr_running is
+ * decreased. We remove the task from the rbtree and
+ * update the mycfs scheduling stats:
+ */
+static void dequeue_task_mycfs(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct sched_entity *se = &p->se;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+
+	dequeue_entity(mycfs_rq, se, flags);
+	mycfs_rq->h_nr_running--;
+	sub_nr_running(rq, 1);
+}
+
+static unsigned long wakeup_gran(struct sched_entity *curr,
+				 struct sched_entity *se)
+{
+	unsigned long gran = sysctl_sched_wakeup_granularity;
+
+	/*
+	 * Since its curr running now, convert the gran from real-time
+	 * to virtual-time in his units.
+	 *
+	 * By using 'se' instead of 'curr' we penalize light tasks, so
+	 * they get preempted easier. That is, if 'se' < 'curr' then
+	 * the resulting gran will be larger, therefore penalizing the
+	 * lighter, if otoh 'se' > 'curr' then the resulting gran will
+	 * be smaller, again penalizing the lighter task.
+	 *
+	 * This is especially important for buddies when the leftmost
+	 * task is higher priority than the buddy.
+	 */
+	return calc_delta_mycfs(gran, se);
+}
+
+/*
+ * Should 'se' preempt 'curr'.
+ *
+ *             |s1
+ *        |s2
+ *   |s3
+ *         g
+ *      |<--->|c
+ *
+ *  w(c, s1) = -1
+ *  w(c, s2) =  0
+ *  w(c, s3) =  1
+ *
+ */
+static int wakeup_preempt_entity(struct sched_entity *curr,
+				 struct sched_entity *se)
+{
+	s64 gran, vdiff = curr->vruntime - se->vruntime;
+
+	if (vdiff <= 0)
+		return -1;
+
+	gran = wakeup_gran(curr, se);
+	if (vdiff > gran)
+		return 1;
+
+	return 0;
+}
+
+/*
+ * Pick the next process, keeping these things in mind, in this order:
+ * 1) keep things fair between processes/task groups
+ * 2) pick the "next" process, since someone really wants that to run
+ * 3) pick the "last" process, for cache locality
+ * 4) do not run the "skip" process, if something else is available
+ */
+static struct sched_entity *pick_next_entity(struct mycfs_rq *mycfs_rq,
+					     struct sched_entity *curr)
+{
+	struct sched_entity *left = __pick_first_entity_mycfs(mycfs_rq);
+	struct sched_entity *se;
+
+	/*
+	 * If curr is set we have to see if its left of the leftmost entity
+	 * still in the tree, provided there was anything in the tree at all.
+	 */
+	if (!left || (curr && entity_before(curr, left)))
+		left = curr;
+
+	/* ideally we run the leftmost entity */
+	se = left;
+
+	/*
+	 * Avoid running the skip buddy, if running something else can
+	 * be done without getting too unfair.
+	 */
+	if (mycfs_rq->skip == se) {
+		struct sched_entity *second;
+
+		if (se == curr) {
+			second = __pick_first_entity_mycfs(mycfs_rq);
+		} else {
+			second = __pick_next_entity(se);
+			if (!second || (curr && entity_before(curr, second)))
+				second = curr;
+		}
+
+		if (second && wakeup_preempt_entity(second, left) < 1)
+			se = second;
+	}
+
+	/*
+	 * Prefer last buddy, try to return the CPU to a preempted task.
+	 */
+	if (mycfs_rq->last && wakeup_preempt_entity(mycfs_rq->last, left) < 1)
+		se = mycfs_rq->last;
+
+	/*
+	 * Someone really wants this to run. If it's not unfair, run it.
+	 */
+	if (mycfs_rq->next && wakeup_preempt_entity(mycfs_rq->next, left) < 1)
+		se = mycfs_rq->next;
+
+	clear_buddies(mycfs_rq, se);
+
+	return se;
+}
+
+/*
+ * Optional action to be done while updating the load average
+ */
+#define UPDATE_TG	0x1
+#define SKIP_AGE_LOAD	0x2
+
+/* Update task and its mycfs_rq load average */
+static inline void update_load_avg(struct sched_entity *se, int flags)
+{
+}
+
+/*
+ * update_stats_curr_start
+ * We are picking a new current task - update its stats:
+ */
+static inline void update_stats_curr_start(struct mycfs_rq *mycfs_rq,
+					   struct sched_entity *se)
+{
+	/* We are starting a new run period: */
+	se->exec_start = rq_clock_task(rq_of(mycfs_rq));
+}
+
+static void put_prev_entity(struct mycfs_rq *mycfs_rq, struct sched_entity *prev)
+{
+	mycfs_printk("pid:%d,se->on_rq:%d",
+		task_of(prev)->pid, prev->on_rq);
+
+	if (prev->on_rq) {
+		/*
+		 * If still on the runqueue then deactivate_task()
+		 * was not called and update_curr() has to be done:
+		 */
+		update_curr(mycfs_rq);
+
+		/* Put 'current' back into the tree. */
+		__enqueue_entity(mycfs_rq, prev);
+	}
+	mycfs_rq->curr = NULL;
+}
+
+static void set_next_entity(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	/* 'current' is not kept within the tree. */
+	if (se->on_rq) {
+		__dequeue_entity(mycfs_rq, se);
+		update_load_avg(se, UPDATE_TG);
+	}
+
+	update_stats_curr_start(mycfs_rq, se);
+	mycfs_rq->curr = se;
+
+	se->prev_sum_exec_runtime = se->sum_exec_runtime;
+}
+
+static struct task_struct *pick_next_task_mycfs(struct rq *rq,
+						struct task_struct *prev,
+						struct pin_cookie cookie)
+{
+	struct mycfs_rq *mycfs_rq = &rq->mycfs;
+	struct sched_entity *se;
+	struct task_struct *p;
+
+	if (!mycfs_rq->nr_running)
+		return NULL;
+
+	put_prev_task(rq, prev);
+
+	se = pick_next_entity(mycfs_rq, NULL);
+	set_next_entity(mycfs_rq, se);
+	p = task_of(se);
+
+	mycfs_printk("pid:%d,nr_running:%d",
+		p->pid, mycfs_rq->nr_running);
+
+	return p;
+}
+
+/*
+ * sched_yield() is very simple
+ *
+ * The magic of dealing with the ->skip buddy is in pick_next_entity.
+ */
+static void yield_task_mycfs(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	struct mycfs_rq *mycfs_rq = task_mycfs_rq(curr);
+	struct sched_entity *se = &curr->se;
+
+	/* Are we the only task in the tree? */
+	if (unlikely(rq->nr_running == 1))
+		return;
+
+	clear_buddies(mycfs_rq, se);
+
+	update_rq_clock(rq);
+
+	/* Update run-time statistics of the 'current' */
+	update_curr(mycfs_rq);
+
+	/*
+	 * Tell update_rq_clock() that we've just updated,
+	 * so we don't do microscopic update in schedule()
+	 * and double the fastpath cost.
+	 */
+	rq_clock_skip_update(rq, true);
+
+	set_skip_buddy(se);
+}
+
+static bool yield_to_task_mycfs(struct rq *rq, struct task_struct *p, bool preempt)
+{
+	struct sched_entity *se = &p->se;
+
+	if (!se->on_rq)
+		return false;
+
+	/* Tell the scheduler that we'd really like pse to run next. */
+	set_next_buddy(se);
+
+	yield_task_mycfs(rq);
+
+	return true;
+}
+
+/*
+ * Called from wake_up_new_task().
+ * Preempt the current task with a newly woken task if needed:
+ */
+static void check_preempt_wakeup_mycfs(struct rq *rq, struct task_struct *p,
+				       int wake_flags)
+{
+	struct task_struct *curr = rq->curr;
+	struct sched_entity *se = &curr->se, *pse = &p->se;
+
+	if (unlikely(se == pse))
+		return;
+
+	/*
+	 * We can come here with TIF_NEED_RESCHED already set from new task
+	 * wake up path.
+	 */
+	if (test_tsk_need_resched(curr))
+		return;
+
+	update_curr(mycfs_rq_of(se));
+
+	if (wakeup_preempt_entity(se, pse) == 1) {
+		mycfs_printk("pid: %d preempt current: %d",
+			p->pid, curr->pid);
+		resched_curr(rq);
+	}
+}
+
+/*
+ * Called periodically by scheduler tick in HZ frequency
+ * Check if we need to preempt current task:
+ */
+static void check_preempt_tick(struct mycfs_rq *mycfs_rq, struct sched_entity *curr)
+{
+
+	/*
+	 * Just do nothing, we do not want any preemption here
+	 */
+}
+
+/*
+ * put_prev_task_mycfs: account for a descheduled task.
+ */
+static void put_prev_task_mycfs(struct rq *rq, struct task_struct *prev)
+{
+	struct sched_entity *se;
+	struct mycfs_rq *mycfs_rq;
+
+	se = &prev->se;
+	mycfs_rq = mycfs_rq_of(se);
+	put_prev_entity(mycfs_rq, se);
+}
+
+#ifdef CONFIG_SMP
+static int select_task_rq_mycfs(struct task_struct *p, int prev_cpu,
+				int sd_flag, int wake_flags)
+{
+	/* No migration now */
+	return task_cpu(p);
+}
+
+/*
+ * task_dead_mycfs
+ * Callback from finish_task_switch, after a task set its state to TASK_DEAD,
+ * which happens after a task do_exit():
+ */
+static void task_dead_mycfs(struct task_struct *p)
+{
+	mycfs_printk("pid:%d,comm:%s", p->pid, p->comm);
+}
+#endif /* CONFIG_SMP */
+
+/*
+ * Account for a task changing its policy
+ *
+ * This routine is mostly called to set mycfs_rq->curr field when a task
+ * migrates between sched classes.
+ */
+static void set_curr_task_mycfs(struct rq *rq)
+{
+	struct sched_entity *se;
+	struct mycfs_rq *mycfs_rq;
+
+	se = &rq->curr->se;
+	mycfs_rq = mycfs_rq_of(se);
+	set_next_entity(mycfs_rq, se);
+
+	mycfs_printk("pid:%d", task_of(se)->pid);
+}
+
+/**
+ * task_tick_mycfs
+ * scheduler tick hitting a task of our scheduling class:
+ */
+static void task_tick_mycfs(struct rq *rq, struct task_struct *curr, int queued)
+{
+	struct sched_entity *se = &curr->se;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+
+	/* Update run-time statistics of the 'current' */
+	update_curr(mycfs_rq);
+
+	if (mycfs_rq->nr_running > 1)
+		check_preempt_tick(mycfs_rq, &curr->se);
+}
+
+/*
+ * Called on fork with the child task as argument from the parent's context
+ *  - child not yet on the tasklist
+ *  - preemption disabled
+ */
+static void task_fork_mycfs(struct task_struct *p)
+{
+	struct mycfs_rq *mycfs_rq;
+	struct sched_entity *curr, *se = &p->se;
+	struct rq *rq = this_rq();
+
+	raw_spin_lock(&rq->lock);
+	update_rq_clock(rq);
+
+	mycfs_rq = task_mycfs_rq(current);
+	curr = mycfs_rq->curr;
+	if (curr) {
+		update_curr(mycfs_rq);
+		se->vruntime = curr->vruntime;
+	}
+	place_entity(mycfs_rq, se, 1);
+
+	if (sysctl_sched_child_runs_first && curr && entity_before(curr, se)) {
+		/*
+		 * Upon rescheduling, sched_class::put_prev_task() will place
+		 * 'current' within the tree based on its new key value.
+		 */
+		swap(curr->vruntime, se->vruntime);
+		resched_curr(rq);
+	}
+
+	/* Inherit period limit */
+	se->mycfs_limit = current->se.mycfs_limit;
+
+	se->vruntime -= mycfs_rq->min_vruntime;
+	raw_spin_unlock(&rq->lock);
+
+	mycfs_printk("P-p:%d/l:%Lu/vrt:%Lu, C-p:%d/l:%Lu/vrt:%Lu",
+		current->pid, current->se.mycfs_limit, current->se.vruntime,
+		p->pid, p->se.mycfs_limit, p->se.vruntime);
+}
+
+/*
+ * Priority of the task has changed. Check to see if we preempt
+ * the current task.
+ */
+static void prio_changed_mycfs(struct rq *rq, struct task_struct *p, int oldprio)
+{
+	if (!task_on_rq_queued(p))
+		return;
+
+	/*
+	 * Reschedule if we are currently running on this runqueue and
+	 * our priority decreased, or if we are not currently running on
+	 * this runqueue and our priority is higher than the current's
+	 */
+	if (rq->curr == p) {
+		if (p->prio > oldprio)
+			resched_curr(rq);
+	} else
+		check_preempt_curr(rq, p, 0);
+}
+
+static inline bool vruntime_normalized(struct task_struct *p)
+{
+	struct sched_entity *se = &p->se;
+
+	/*
+	 * In both the TASK_ON_RQ_QUEUED and TASK_ON_RQ_MIGRATING cases,
+	 * the dequeue_entity(.flags=0) will already have normalized the
+	 * vruntime.
+	 */
+	if (p->on_rq)
+		return true;
+
+	/*
+	 * When !on_rq, vruntime of the task has usually NOT been normalized.
+	 * But there are some cases where it has already been normalized:
+	 *
+	 * - A forked child which is waiting for being woken up by
+	 *   wake_up_new_task().
+	 * - A task which has been woken up by try_to_wake_up() and
+	 *   waiting for actually being woken up by sched_ttwu_pending().
+	 */
+	if (!se->sum_exec_runtime || p->state == TASK_WAKING)
+		return true;
+
+	return false;
+}
+
+/**
+ * switched_from_mycfs
+ * Callback function when task @p changed its sched class from MyCFS
+ */
+static void switched_from_mycfs(struct rq *rq, struct task_struct *p)
+{
+	struct sched_entity *se = &p->se;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+
+	cpufreq_update_util(rq, 0);
+
+	if (!vruntime_normalized(p)) {
+		/*
+		 * Fix up our vruntime so that the current sleep doesn't
+		 * cause 'unlimited' sleep bonus.
+		 */
+		place_entity(mycfs_rq, se, 0);
+		se->vruntime -= mycfs_rq->min_vruntime;
+	}
+}
+
+/**
+ * switched_to_mycfs
+ * Callback function when task @p changed its sched class to MyCFS
+ */
+static void switched_to_mycfs(struct rq *rq, struct task_struct *p)
+{
+	struct sched_entity *se = &p->se;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+
+	/* Take a note about CPU utilization changes */
+	cpufreq_update_util(rq, 0);
+
+	if (!vruntime_normalized(p))
+		se->vruntime += mycfs_rq->min_vruntime;
+
+	if (task_on_rq_queued(p)) {
+		/*
+		 * We were most likely switched from sched_cfs, so
+		 * kick off the schedule if running, otherwise just see
+		 * if we can still preempt the current task:
+		 */
+		if (rq->curr == p)
+			resched_curr(rq);
+		else
+			check_preempt_curr(rq, p, 0);
+	}
+}
+
+static unsigned int get_rr_interval_mycfs(struct rq *rq,
+					  struct task_struct *task)
+{
+	return 0;
+}
+
+const struct sched_class mycfs_sched_class = {
+	.next			= &idle_sched_class,
+
+	.enqueue_task		= enqueue_task_mycfs,
+	.dequeue_task		= dequeue_task_mycfs,
+
+	.yield_task		= yield_task_mycfs,
+	.yield_to_task		= yield_to_task_mycfs,
+
+	.check_preempt_curr	= check_preempt_wakeup_mycfs,
+
+	.pick_next_task		= pick_next_task_mycfs,
+	.put_prev_task		= put_prev_task_mycfs,
+
+#ifdef CONFIG_SMP
+	/* task migration callback: */
+	.select_task_rq		= select_task_rq_mycfs,
+
+	.set_cpus_allowed	= set_cpus_allowed_common,
+#endif
+
+	/* task exit point callback: */
+	.task_dead		= task_dead_mycfs,
+
+	.set_curr_task          = set_curr_task_mycfs,
+
+	/* Called with HZ frequency by scheduler tick */
+	.task_tick		= task_tick_mycfs,
+
+	/* fock()-time callback: */
+	.task_fork		= task_fork_mycfs,
+
+	/*
+	 * Called when sched class changed
+	 * (via check_class_changed() only):
+	 */
+	.prio_changed		= prio_changed_mycfs,
+	.switched_from		= switched_from_mycfs,
+	.switched_to		= switched_to_mycfs,
+
+	/* Return the default timeslice of a process */
+	.get_rr_interval	= get_rr_interval_mycfs,
+
+	.update_curr		= update_curr_mycfs,
+};
+
+void init_mycfs_rq(struct mycfs_rq *mycfs_rq)
+{
+	mycfs_rq->tasks_timeline = RB_ROOT;
+	mycfs_rq->min_vruntime = 0;
+	mycfs_rq->period = MYCFS_PREIOD;
+
+	printk_once("MyCFS period: %Lu ns", MYCFS_PREIOD);
+}
+
+static int do_sched_setlimit(struct task_struct *p, u64 limit)
+{
+	p->se.mycfs_limit = limit;
+
+	return 0;
+}
+
+SYSCALL_DEFINE2(sched_setlimit, pid_t, pid, int, limit)
+{
+	struct task_struct *p;
+	int ret;
+
+	if (pid < 0 || limit < 0 || limit >= (MYCFS_PREIOD/1000))
+		return -EINVAL;
+
+	ret = -ESRCH;
+	p = pid ? find_task_by_vpid(pid) : current;
+	if (p) {
+		pr_info("%s(CPU%d): %d/%s, limit: %Lu",
+			__func__, smp_processor_id(), p->pid, p->comm, limit*1000);
+
+		ret = do_sched_setlimit(p, limit * 1000);
+	}
+	return ret;
+}
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/sched.h b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/sched.h
index 7b34c78..1222476 100644
--- a/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/sched.h
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/kernel/sched/sched.h
@@ -29,6 +29,8 @@ struct cpuidle_state;
 #define TASK_ON_RQ_QUEUED	1
 #define TASK_ON_RQ_MIGRATING	2
 
+extern unsigned int sched_nr_latency;
+
 extern __read_mostly int scheduler_running;
 
 extern unsigned long calc_load_update;
@@ -117,10 +119,17 @@ static inline int dl_policy(int policy)
 {
 	return policy == SCHED_DEADLINE;
 }
+
+static inline int mycfs_policy(int policy)
+{
+	return policy == SCHED_MYCFS;
+}
+
 static inline bool valid_policy(int policy)
 {
 	return idle_policy(policy) || fair_policy(policy) ||
-		rt_policy(policy) || dl_policy(policy);
+		rt_policy(policy) || dl_policy(policy) ||
+		mycfs_policy(policy);
 }
 
 static inline int task_has_rt_policy(struct task_struct *p)
@@ -133,6 +142,11 @@ static inline int task_has_dl_policy(struct task_struct *p)
 	return dl_policy(p->policy);
 }
 
+static inline int task_has_mycfs_policy(struct task_struct *p)
+{
+	return mycfs_policy(p->policy);
+}
+
 /*
  * Tells if entity @a should preempt entity @b.
  */
@@ -371,6 +385,27 @@ struct cfs_bandwidth { };
 
 #endif	/* CONFIG_CGROUP_SCHED */
 
+/* MyCFS-related fields in a runqueue */
+struct mycfs_rq {
+	struct load_weight load;
+	unsigned int nr_running, h_nr_running;
+
+	u64 period;
+	u64 exec_clock;
+
+	/* minimum run time of any task in the MyCFS runqueue */
+	u64 min_vruntime;
+
+	struct rb_root tasks_timeline;
+	struct rb_node *rb_leftmost;
+
+	/*
+	 * 'curr' points to currently running entity on this cfs_rq.
+	 * It is set to NULL otherwise (i.e when none are currently running).
+	 */
+	struct sched_entity *curr, *next, *last, *skip;
+};
+
 /* CFS-related fields in a runqueue */
 struct cfs_rq {
 	struct load_weight load;
@@ -622,6 +657,7 @@ struct rq {
 	unsigned long nr_load_updates;
 	u64 nr_switches;
 
+	struct mycfs_rq mycfs;
 	struct cfs_rq cfs;
 	struct rt_rq rt;
 	struct dl_rq dl;
@@ -1307,6 +1343,7 @@ extern const struct sched_class stop_sched_class;
 extern const struct sched_class dl_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
+extern const struct sched_class mycfs_sched_class;
 extern const struct sched_class idle_sched_class;
 
 
@@ -1712,7 +1749,11 @@ static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)
 extern struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq);
 extern struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq);
 
+struct sched_entity *__pick_first_entity_mycfs(struct mycfs_rq *mycfs_rq);
+struct sched_entity *__pick_last_entity_mycfs(struct mycfs_rq *mycfs_rq);
+
 #ifdef	CONFIG_SCHED_DEBUG
+extern void print_mycfs_stats(struct seq_file *m, int cpu);
 extern void print_cfs_stats(struct seq_file *m, int cpu);
 extern void print_rt_stats(struct seq_file *m, int cpu);
 extern void print_dl_stats(struct seq_file *m, int cpu);
@@ -1728,6 +1769,7 @@ print_numa_stats(struct seq_file *m, int node, unsigned long tsf,
 #endif /* CONFIG_NUMA_BALANCING */
 #endif /* CONFIG_SCHED_DEBUG */
 
+extern void init_mycfs_rq(struct mycfs_rq *mycfs_rq);
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq);
 extern void init_dl_rq(struct dl_rq *dl_rq);
diff --git a/ECE695-Spring17/t3-scheduler/linux-4.10.6/xxx.sh b/ECE695-Spring17/t3-scheduler/linux-4.10.6/xxx.sh
new file mode 100755
index 0000000..c2b3cc2
--- /dev/null
+++ b/ECE695-Spring17/t3-scheduler/linux-4.10.6/xxx.sh
@@ -0,0 +1 @@
+make -j 8 bzImage && make install && sync
